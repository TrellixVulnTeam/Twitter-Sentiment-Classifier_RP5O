{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Avyakta\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "import re \n",
    "\n",
    "# from data_cleaning import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading training data\n",
    "data = pd.read_csv('data_twitter_sentiment/semeval_train.txt',sep='\\t',names=[\"sentiment\",\"tweet\"])\n",
    "# data.head()\n",
    "# reading and preparing test data\n",
    "dt1 = pd.read_csv('data_twitter_sentiment/Twitter2013_raw.txt',sep='\\t',names=[\"sentiment\",\"tweet\"])\n",
    "dt2 = pd.read_csv('data_twitter_sentiment/Twitter2014_raw.txt',sep='\\t',names=[\"sentiment\",\"tweet\"])\n",
    "dt3 = pd.read_csv('data_twitter_sentiment/Twitter2015_raw.txt',sep='\\t',names=[\"sentiment\",\"tweet\"])\n",
    "dt4 = pd.read_csv('data_twitter_sentiment/Twitter2016_raw.txt',sep='\\t',names=[\"sentiment\",\"tweet\"])\n",
    "\n",
    "# dt = pd.concat([dt1, dt2, dt3, dt4])\n",
    "dt = dt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(features):\n",
    "    processed_features = []\n",
    "    for sentence in range(0, len(features)):\n",
    "        # remove hyperlinks, tags, hashtags\n",
    "        processed_feature = ' ' + str(features[sentence]) + ' '\n",
    "        processed_feature = re.sub(r'http*\\S+', ' ', processed_feature) \n",
    "        processed_feature = re.sub(r'https*\\S+', ' ', processed_feature)\n",
    "        processed_feature = re.sub(r'@\\S+', ' ', processed_feature)\n",
    "        processed_feature = re.sub(r'#\\S+', ' ', processed_feature)\n",
    "        processed_feature = re.sub(r'\\bhm*\\s+', '', processed_feature)\n",
    "        \n",
    "        # remove all digits\n",
    "        processed_feature = re.sub(r'[0-9]', ' ', processed_feature)\n",
    "        processed_feature = re.sub(r'[_]', ' ', processed_feature)\n",
    "        \n",
    "        # Remove all the special characters\n",
    "        processed_feature = re.sub(r'\\W', ' ', processed_feature)\n",
    "        \n",
    "        # remove all single characters\n",
    "        processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        processed_feature = processed_feature.lower()\n",
    "\n",
    "        # remove \n",
    "        processed_feature = re.sub(r'(\\s)aa\\w+', ' ', processed_feature)\n",
    "        processed_feature = re.sub(r'(\\s)ba(\\s)', ' ', processed_feature)\n",
    "        processed_feature = re.sub(r'(\\s)th(\\s)', ' ', processed_feature)\n",
    "\n",
    "        processed_features.append(processed_feature)\n",
    "    return processed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8588, 17797)\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# data cleaning\n",
    "train_features = preprocess(data.iloc[:, 1].values)\n",
    "test_features = preprocess(dt.iloc[:, 1].values)\n",
    "\n",
    "# vectorizing data\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df=2, max_df=0.95, norm = 'l2',ngram_range=(1, 4)).fit(train_features)\n",
    "# v = vectorizer\n",
    "cv_array = vectorizer.transform(train_features)\n",
    "cvt_array = vectorizer.transform(test_features)\n",
    "\n",
    "print( cv_array.shape, sep='\\n')\n",
    "\n",
    "# label encoding\n",
    "lab_enc = preprocessing.LabelEncoder().fit(data['sentiment'])\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x_train, x_test,label_train, label_test = train_test_split(cv_array, data['sentiment'], test_size=0.2, random_state=0)\n",
    "x_train = cv_array\n",
    "label_train = data['sentiment']\n",
    "y_train = lab_enc.transform(label_train)\n",
    "\n",
    "x_test = cvt_array\n",
    "label_test = dt['sentiment']\n",
    "y_test = lab_enc.transform(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "x_train = torch.tensor(scipy.sparse.csr_matrix.todense(x_train)).float()\n",
    "x_test = torch.tensor(scipy.sparse.csr_matrix.todense(x_test)).float()\n",
    "y_train = torch.tensor(y_train).type(torch.LongTensor)\n",
    "y_test = torch.tensor(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loss is tensor(1.1027)  and train accuracy is tensor(0.1469)\n",
      "Test  loss is tensor(1.0028)  and test  accuracy is tensor(0.3612)\n",
      "\n",
      "\n",
      "Train loss is tensor(0.7531)  and train accuracy is tensor(0.6701)\n",
      "Test  loss is tensor(0.8289)  and test  accuracy is tensor(0.5956)\n",
      "\n",
      "\n",
      "Train loss is tensor(0.6534)  and train accuracy is tensor(0.7111)\n",
      "Test  loss is tensor(0.8593)  and test  accuracy is tensor(0.6042)\n",
      "\n",
      "\n",
      "Train loss is tensor(0.5398)  and train accuracy is tensor(0.8331)\n",
      "Test  loss is tensor(0.8794)  and test  accuracy is tensor(0.6204)\n",
      "\n",
      "\n",
      "Train loss is tensor(0.5116)  and train accuracy is tensor(0.8347)\n",
      "Test  loss is tensor(0.8976)  and test  accuracy is tensor(0.6188)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "             nn.Linear(x_train.shape[1], 128),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(128, 128),\n",
    "            #  nn.ReLU(),\n",
    "            #  nn.Linear(128, 128),\n",
    "             nn.Linear(128, 3),\n",
    "             nn.LogSoftmax(dim=1))\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "# Forward pass, log  \n",
    "logps = model(x_train)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logps, y_train)\n",
    "loss.backward()\n",
    "# Optimizers need parameters to optimize and a learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay = 0.009)\n",
    "\n",
    "epochs = 201\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model.forward(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    if e % 50 == 0:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            log_ps = model(x_test)\n",
    "            test_loss = criterion(log_ps, y_test)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == y_test.view(*top_class.shape)\n",
    "            test_accuracy = torch.mean(equals.float())\n",
    "\n",
    "            ps = torch.exp(output)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == y_train.view(*top_class.shape)\n",
    "            train_accuracy = torch.mean(equals.float())\n",
    "        print(\"Train loss is\",loss.detach(), \" and train accuracy is\", train_accuracy)\n",
    "        print(\"Test  loss is\",test_loss, \" and test  accuracy is\", test_accuracy)\n",
    "        print('\\n')\n",
    "# evaluation after every 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.5652, -1.3220, -0.6456],\n",
       "        [-1.5592, -0.7301, -1.1782],\n",
       "        [-1.6930, -1.8325, -0.4216],\n",
       "        ...,\n",
       "        [-2.0599, -0.2250, -2.6035],\n",
       "        [-1.9571, -2.5760, -0.2451],\n",
       "        [-1.6469, -1.6752, -0.4779]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model(x_train)"
   ]
  }
 ]
}
